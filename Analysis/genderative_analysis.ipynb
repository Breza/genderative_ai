{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3aee222c12a87d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis plan\n",
    "\n",
    "* Load training data\n",
    "* Make sure classes are evenly balanced, otherwise throw exception\n",
    "* Load validation data\n",
    "* Train model on training data\n",
    "* Test model on validation data\n",
    "* Move uncertain images from training and holdout sets to review folder\n",
    "* Manually review uncertain images\n",
    "* Adjust images as needed (delete bad images and move misclassified good images to training set)\n",
    "* Calculate validation metrics for each occupation\n",
    "* Create more training data if some occupations have bad metrics\n",
    "* Repeat as needed\n",
    "* Run final model against holdout data that hasn't been previously examined\n",
    "* Calculate overall metrics and break out by gender and occupation\n",
    "* Run final model against unlabeled data\n",
    "* Save prediction for each image\n",
    "* Calculate gender representation for each occupation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "100e3aeb9e982dad"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPU available, falling back to CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:22:50.221819500Z",
     "start_time": "2024-01-03T15:22:43.975709800Z"
    }
   },
   "id": "d463fb008caa33fe"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "# Functions\n",
    "def current_time_only(file_safe: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time at second precision without date.\n",
    "\n",
    "    Parameters:\n",
    "    file_safe (bool): If True, returns time formatted for file naming (replaces ':' with '_').\n",
    "\n",
    "    Returns:\n",
    "    str: Current time formatted as 'HH:MM:SS' or 'DD_HH_MM_SS' if file_safe is True.\n",
    "    \"\"\"\n",
    "    if file_safe:\n",
    "        return datetime.datetime.now().strftime('%d:%H:%M:%S').replace(\":\", \"_\")\n",
    "    else:\n",
    "        return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "def count_files_in_directory(path: Union[str, 'LiteralString']) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of files in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "    path (Union[str, 'LiteralString']): The file path to the directory whose contents are to be counted.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If path is not a directory or does not exist.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of files in the specified directory.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"{path} is not a directory.\")\n",
    "    return len(os.listdir(path))\n",
    "\n",
    "\n",
    "def plot_training_progress(train_acc: list,\n",
    "                           train_loss: list,\n",
    "                           val_acc: list,\n",
    "                           validation_loss: list,\n",
    "                           title=\"Model results\"):\n",
    "    \"\"\"\n",
    "    Plot training-vs-testing accuracy and loss for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        train_acc (list): List of training accuracy values from each epoch, must be same length as val_acc\n",
    "        train_loss (list): List of training loss values from each epoch, must be same length as val_loss\n",
    "        val_acc (list): List of validation accuracy values from each epoch, must be same length as train_acc\n",
    "        validation_loss (list): List of validation loss values from each epoch, must be same length as train_loss\n",
    "        title (str): Plot title\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of accuracy or loss lists don't match each other\n",
    "    \"\"\"\n",
    "    if not (len(train_acc) == len(val_acc) and len(train_loss) == len(val_loss)) and len(train_acc) > 0 and len(\n",
    "            train_loss) > 0:\n",
    "        raise ValueError(\"Lengths of training and validation lists must match\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    axs[0].plot(train_acc,\n",
    "                label=\"Training accuracy\",\n",
    "                color=train_color,\n",
    "                linewidth=3)\n",
    "    axs[0].plot(val_acc,\n",
    "                label=\"Validation accuracy\",\n",
    "                color=val_color,\n",
    "                linewidth=3)\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(train_loss,\n",
    "                label=\"Training loss\",\n",
    "                color=train_color,\n",
    "                linewidth=3)\n",
    "    axs[1].plot(validation_loss,\n",
    "                label=\"Validation loss\",\n",
    "                color=val_color,\n",
    "                linewidth=3)\n",
    "    axs[1].set_title(\"Loss\")\n",
    "    axs[1].set(ylim=(0, None))\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def label_images(csv_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads images from a specified CSV file and allows the user to label them interactively.\n",
    "\n",
    "    This function opens each image specified in the CSV file and displays it to the user. \n",
    "    The user can then label the image as 'Female', 'Male', or 'Discard' by pressing the corresponding \n",
    "    keys ('f', 'm', 'd' or '0', '1', '2'). The function logs the user's decision along with the image \n",
    "    path and writes this data to an output file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing the paths of the images to be labeled.\n",
    "    output_file (str): Path to the output CSV file where the image paths and labels will be saved.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If a key pressed is not among the specified keys ('f', 'm', 'd', '0', '1', '2').\n",
    "\n",
    "    Notes:\n",
    "    The function adds a text overlay to each image before displaying it, which indicates the keys \n",
    "    for labeling. If an image cannot be read, it logs the path with the label \"ERROR\".\n",
    "    \"\"\"\n",
    "    image_labels = []\n",
    "    image_files = pl.read_csv(csv_file).to_series().to_list()\n",
    "\n",
    "    for image_path in image_files:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            cv2.putText(image, \"'F'emale, 'M'ale, 'D'iscard\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Image\", image)\n",
    "\n",
    "            key = cv2.waitKey(0)  # Wait for a key press\n",
    "            if key == ord('0') or key == ord('f'):\n",
    "                image_labels.append((image_path, \"Female\"))\n",
    "            elif key == ord('1') or key == ord('m'):\n",
    "                image_labels.append((image_path, \"Male\"))\n",
    "            elif key == ord('2') or key == ord('d'):\n",
    "                image_labels.append((image_path, \"Discard\"))\n",
    "            else:\n",
    "                raise ValueError(\"Only allowed values are 0, 1, 2, f, m, and d\")\n",
    "            print(f\"Key pressed: {chr(key)}\")\n",
    "\n",
    "            cv2.destroyAllWindows()\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            image_labels.append((image_path, \"ERROR\"))\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in image_labels:\n",
    "            f.write(f\"{item[0]},{item[1]}\\n\")\n",
    "\n",
    "\n",
    "def calculate_normalization(custom_normalization: bool) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of the dataset for normalization.\n",
    "\n",
    "    Parameters:\n",
    "    custom_normalization (bool): Determines if custom normalization values are to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, List[float]]: A dictionary containing 'mean' and 'std', each a list of three floats.\n",
    "\n",
    "    Notes:\n",
    "    Function uses global variables batch_size and data_dir.\n",
    "    \"\"\"\n",
    "    if custom_normalization:\n",
    "        tensor_transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Create a dataset without normalization\n",
    "        unnormalized_dataset = ImageFolder(os.path.join(data_dir, 'Labeled'), transform=tensor_transform)\n",
    "        unnormalized_loader = DataLoader(unnormalized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        def calculate_mean_std(loader: DataLoader) -> (torch.Tensor, torch.Tensor):\n",
    "            \"\"\"\n",
    "            Calculate the mean and standard deviation of images in a DataLoader.\n",
    "        \n",
    "            Parameters:\n",
    "            loader (DataLoader): The DataLoader containing the dataset.\n",
    "        \n",
    "            Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Mean and standard deviation tensors.\n",
    "            \"\"\"\n",
    "            mean_accumulator = 0.0\n",
    "            variance_accumulator = 0.0\n",
    "            for images, _ in loader:\n",
    "                batch_samples = images.size(0)\n",
    "                images = images.view(batch_samples, images.size(1), -1)\n",
    "                mean_accumulator += images.mean(2).sum(0)\n",
    "                variance_accumulator += images.var(2).sum(0)\n",
    "\n",
    "            mean_accumulator /= len(loader.dataset)\n",
    "            std_deviation = torch.sqrt(variance_accumulator / len(loader.dataset))\n",
    "            return mean_accumulator, std_deviation\n",
    "\n",
    "        dataset_mean, dataset_std = calculate_mean_std(unnormalized_loader)\n",
    "        normalization_dict = {'mean': dataset_mean.tolist(), 'std': dataset_std.tolist()}\n",
    "        del unnormalized_dataset, unnormalized_loader\n",
    "        print(f\"Custom normalization values: {normalization_dict}\")\n",
    "    else:\n",
    "        normalization_dict = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "        print(\"Standard normalization values used\")\n",
    "    return normalization_dict\n",
    "\n",
    "\n",
    "def display_images_from_dataloader(dataloader: DataLoader, num_images: int = 8):\n",
    "    \"\"\"\n",
    "    Fetches a batch of images from the given DataLoader and displays them to user.\n",
    "\n",
    "    Parameters:\n",
    "    dataloader (DataLoader): A PyTorch DataLoader object from which to fetch the images.\n",
    "    num_images (int): The number of images to display from the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def imshow(img: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Display an image by denormalizing and clipping its values.\n",
    "\n",
    "        This function takes a PyTorch tensor representing a grid of images, which have been normalized \n",
    "        previously, and performs denormalization to convert them back to their original color \n",
    "        space. It then clips the image values to be within the range [0, 1] to ensure \n",
    "        proper display. The image is displayed using matplotlib.\n",
    "\n",
    "        Parameters:\n",
    "        img (torch.Tensor): A PyTorch tensor representing a grid of images.\n",
    "        \"\"\"\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = normalization_values['std'] * img + normalization_values['mean']  # Denormalize\n",
    "        img = np.clip(img, 0, 1)  # Clip values to be between 0 and 1\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "    num_images = min(batch_size, num_images)\n",
    "    dataset_images, dataset_labels = next(iter(dataloader))\n",
    "    images_subset = dataset_images[:num_images]\n",
    "    imshow(torchvision.utils.make_grid(images_subset))\n",
    "    print(' '.join(f'{classes[dataset_labels[j]]:5s}' for j in range(num_images)))\n",
    "\n",
    "\n",
    "def predict_model_resnet(my_dataloader):\n",
    "    \"\"\"\n",
    "    Performs predictions using a ResNet model on a given dataloader.\n",
    "    \n",
    "    This function runs the ResNet model in evaluation mode and generates predictions for the input data. \n",
    "    It requires the dataloader to have a batch size of exactly 1. The function outputs a dataframe \n",
    "    with file paths, actual labels, predicted labels, probabilities of being female, and a flag indicating \n",
    "    correct predictions.\n",
    "    \n",
    "    Note: The sorting logic in the function is based on the condition of correct predictions and their confidence scores.\n",
    "    \n",
    "    Parameters:\n",
    "    my_dataloader (Dataloader): A PyTorch Dataloader object containing the data to be predicted. \n",
    "                                 The dataloader must have a batch size of 1.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If the dataloader's batch size is not 1.\n",
    "    \n",
    "    Returns:\n",
    "    Polars.DataFrame: A dataframe containing columns for file paths, labels, predictions, \n",
    "                      probabilities, and correct predictions.\n",
    "    \"\"\"\n",
    "    if my_dataloader.batch_size != 1:\n",
    "        raise ValueError(\"Predictions dataloader requires batch size of exactly 1\")\n",
    "    model_resnet.eval()\n",
    "    prediction_results = pl.DataFrame(schema={\"file_path\": str, \"label\": int, \"prediction\": int, \"prob_female\": float})\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(my_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_resnet(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            prob_female = torch.nn.functional.softmax(outputs, dim=1)[:, 0].tolist()\n",
    "            predicted = predicted.tolist()\n",
    "            labels = labels.tolist()\n",
    "            file_paths = my_dataloader.dataset.samples[i][0]\n",
    "            batch_results = pl.DataFrame({\"file_path\": file_paths,\n",
    "                                          \"label\": labels,\n",
    "                                          \"prediction\": predicted,\n",
    "                                          \"prob_female\": prob_female})\n",
    "            prediction_results = pl.concat([prediction_results, batch_results], how=\"vertical_relaxed\")\n",
    "        prediction_results = prediction_results.with_columns(\n",
    "            (1 * (pl.col(\"label\") == pl.col(\"prediction\"))).alias(\"correct_prediction\")\n",
    "        )\n",
    "        # I couldn't figure out the sorting logic so I asked SO\n",
    "        # https://stackoverflow.com/questions/77700489/how-to-perform-a-conditional-sort-in-polars/77700711\n",
    "        prediction_results = prediction_results.with_columns(\n",
    "            abs(pl.col(\"prob_female\") - 0.5).alias(\"confidence\")\n",
    "        ).sort([\n",
    "            (good_prediction := pl.col('label').eq(pl.col('prediction'))),\n",
    "            (good_prediction - 1) * pl.col('confidence'),\n",
    "            pl.col('confidence')\n",
    "        ])\n",
    "        return prediction_results\n",
    "\n",
    "\n",
    "# Classes\n",
    "class ImbalancedClassesException(ValueError):\n",
    "    \"\"\"Exception raised when classes must be perfectly balanced and are not.\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:22:52.301652200Z",
     "start_time": "2024-01-03T15:22:52.256501900Z"
    }
   },
   "id": "e06b185676f25570"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Config variables\n",
    "train_color: str = \"#ffffb3\"\n",
    "val_color: str = \"#bebada\"\n",
    "unlabeled_color = \"orange\"\n",
    "data_dir: str = 'H:/Photos/AI/'\n",
    "batch_size: int = 30\n",
    "classes: tuple[str, str] = ('Female', 'Male')\n",
    "# TODO: Change before training\n",
    "use_custom_normalization: bool = False\n",
    "num_epochs: int = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:22:55.293339400Z",
     "start_time": "2024-01-03T15:22:55.264894400Z"
    }
   },
   "id": "de3758e1d8c9c592"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc29b10a77bd6120",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:22:57.291696500Z",
     "start_time": "2024-01-03T15:22:57.048050900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of images in each training folder and throw exception if imbalanced\n",
    "file_counts = {category: {gender: count_files_in_directory(\n",
    "    os.path.join(data_dir, category, gender))\n",
    "    for gender in classes}\n",
    "    for category in ['Labeled', 'Holdout']}\n",
    "file_counts.update({'Unlabeled': count_files_in_directory(os.path.join(data_dir, \"Unlabeled/Unlabeled\"))})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Training\\nFemale', 'Training\\nMale',\n",
    "        'Holdout\\nFemale', 'Holdout\\nMale',\n",
    "        'Unlabeled'],\n",
    "       [file_counts['Labeled']['Female'],\n",
    "        file_counts['Labeled']['Male'],\n",
    "        file_counts['Holdout']['Female'],\n",
    "        file_counts['Holdout']['Male'],\n",
    "        file_counts['Unlabeled']],\n",
    "       color=[train_color, train_color, val_color, val_color, unlabeled_color])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Training and validation image counts\")\n",
    "plt.show()\n",
    "\n",
    "if file_counts['Labeled']['Female'] != file_counts['Labeled']['Male']:\n",
    "    female_count = file_counts['Labeled']['Female']\n",
    "    male_count = file_counts['Labeled']['Male']\n",
    "    raise ImbalancedClassesException(\n",
    "        f'Imbalance detected: {female_count} female images vs {male_count} male images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard normalization values used\n"
     ]
    }
   ],
   "source": [
    "normalization_values = calculate_normalization(use_custom_normalization)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:23:00.798123500Z",
     "start_time": "2024-01-03T15:23:00.766228900Z"
    }
   },
   "id": "2774d1fcab1bba50"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load train/test images in Pytorch format\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Images are already 512x512 but it's good to be sure\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalization_values['mean'], std=normalization_values['std'])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(data_dir, 'Labeled'), transform=transformations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = ImageFolder(os.path.join(data_dir, 'Holdout'), transform=transformations)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-03T15:23:05.490603900Z",
     "start_time": "2024-01-03T15:23:05.357443400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_images_from_dataloader(train_loader, num_images=16)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df8bb591a7e6b7cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5734761e01a6b298"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model_resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze the layers except the final layer\n",
    "for param in model_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer for binary classification\n",
    "num_features = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Move model_resnet to GPU (if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_resnet = model_resnet.to(device)\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize the parameters of the final layer\n",
    "optimizer = optim.Adam(model_resnet.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "154b6f22a12e1b9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet_train_epoch_loss = []\n",
    "resnet_train_epoch_accuracy = []\n",
    "resnet_validation_epoch_loss = []\n",
    "resnet_validation_epoch_accuracy = []\n",
    "\n",
    "# Fine-tune the ResNet model\n",
    "for epoch in range(num_epochs):\n",
    "    model_resnet.train()\n",
    "    print(f\"Epoch {epoch + 1} initiated ({current_time_only()})\")\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        if i % 250 == 0:\n",
    "            print(f\"Training batch {i + 1}/{len(train_loader)} ({current_time_only()})\")\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    resnet_train_epoch_loss.append(running_loss / len(train_loader))\n",
    "    resnet_train_epoch_accuracy.append(correct_train / total_train)\n",
    "\n",
    "    # Validation\n",
    "    model_resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        print(f\"Validation started ({current_time_only()})\")\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_resnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and store the validation loss and accuracy for the epoch\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_accuracy = correct / total\n",
    "    resnet_validation_epoch_loss.append(epoch_val_loss)\n",
    "    resnet_validation_epoch_accuracy.append(epoch_val_accuracy)\n",
    "\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(f\"{'*' * 10} EPOCH {epoch + 1} RESULTS {'*' * 10}\".center(50))\n",
    "    print(\"*\" * 50)\n",
    "    print(f'\\033[94mTrain Loss: {running_loss / len(train_loader):.3f}\\033[0m, '\n",
    "          f'\\033[92mValidation Loss: {epoch_val_loss:.3f}\\033[0m, '\n",
    "          f'\\033[93mTraining Accuracy: {100 * correct_train / total_train:.2f}%\\033[0m, '\n",
    "          f'\\033[91mValidation Accuracy: {100 * epoch_val_accuracy:.2f}%\\033[0m')\n",
    "    print(\"*\" * 50 + \"\\n\")\n",
    "\n",
    "print(f'Finished Training ({current_time_only()})')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a91ff7e6685697f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_training_progress(train_acc=resnet_train_epoch_accuracy,\n",
    "                       train_loss=resnet_train_epoch_loss,\n",
    "                       val_acc=resnet_validation_epoch_accuracy,\n",
    "                       val_loss=resnet_validation_epoch_loss,\n",
    "                       title=\"Fine-tuned model results\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "618d925dcbba56e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exploration_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "exploration_results = predict_model_resnet(exploration_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f22dbf89b4f4c49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pie([exploration_results.select(pl.mean(\"correct_prediction\")).to_series().to_list()[0],\n",
    "        1 - exploration_results.select(pl.mean(\"correct_prediction\")).to_series().to_list()[0]],\n",
    "       labels=[\"Correct\", \"Incorrect\"],\n",
    "       autopct=\"%1.1f%%\",\n",
    "       colors=[\"#76ffbf\", \"#ff8876\"])\n",
    "plt.title(\"Cross-validated accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac48ac782a5ae24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in val_dataset.class_to_idx.items()}\n",
    "\n",
    "accuracy_by_gender = (exploration_results.select([\"label\", \"correct_prediction\"])\n",
    ".group_by(\"label\")\n",
    ".mean()\n",
    ".with_columns(\n",
    "    pl.col(\"label\").replace(idx_to_class)\n",
    "))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(list(zip(*accuracy_by_gender.rows()))[0], list(zip(*accuracy_by_gender.rows()))[1], color=\"#76ffbf\")\n",
    "plt.title(\"Accuracy by gender\")\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed7fa36ce328c292"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps\n",
    "* Move incorrect and low-confidence images to separate folder for review\n",
    "* Retrain model as needed\n",
    "* Predict gender for all unlabeled images\n",
    "* Add column for occupation\n",
    "* Move low-confidence predictions to review folder\n",
    "* Group gender breakdowns by occupation\n",
    "* Compare gender breakdowns to BLS data\n",
    "* Plot scatterplot comparing breakdowns (SD vs. BLS)\n",
    "* Write the paper "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6f0e2673b1dda2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_dataset = ImageFolder(os.path.join(data_dir, 'Unlabeled'), transform=tensor_transform)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "unlabeled_predictions = predict_model_resnet(unlabeled_loader)\n",
    "unlabeled_predictions.drop([\"label\", \"correct_prediction\"])\n",
    "unlabeled_predictions = unlabeled_predictions.with_columns(\n",
    "    pl.col(\"file_path\").str.extract(\"(?:[^_]*_){4}([^_]*)_\").alias(\"occupation\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d4025090c232e05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_predictions.sort(\"confidence\", descending=True).head(6).to_series(0)\n",
    "\n",
    "for img_path in unlabeled_predictions.sort(\"confidence\", descending=True).head(6).to_series(0):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    display(PIL.Image.open(img_path).resize((256, 256)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "605989ac63bb83a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(unlabeled_predictions.\n",
    " select([\"occupation\", \"prediction\"]).\n",
    " group_by(\"occupation\").\n",
    " mean().\n",
    " sort(\"prediction\", descending=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85dcff2aa006686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_predictions.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c57ccc73a72b4be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Save all low-confidence images\n",
    "unlabeled_predictions.sort(\"confidence\").head(10).select(\"file_path\").write_csv(\"unlabeled_path_test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d6ec9ceb76741b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_images(\"unlabeled_path_test.csv\", \"labels.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86962fd28671072e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual review\n",
    "* Classify and move low-confidence and incorrect training images within training directory\n",
    "* Classify and move low-confidence and incorrect holdout images within holdout directory\n",
    "* Report holdout classification accuracy by occupation and gender\n",
    "* Classify low-confidence unlabeled images and copy them to the training directory\n",
    "* Make sure training classes remain balanced\n",
    "* Classify a random sample of holdout images to assess model accuracy and inter-rater bias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d64efeb4cc8e19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f48f21c45ca8bf4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
