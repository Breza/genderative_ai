{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Genderative AI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "100e3aeb9e982dad"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "from functools import cache\n",
    "from typing import Dict, List, NamedTuple, Union\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPU available, falling back to CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:22:20.559305800Z",
     "start_time": "2024-02-01T20:21:57.803858100Z"
    }
   },
   "id": "d463fb008caa33fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "# Functions\n",
    "def current_time_only(file_safe: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time at second precision without date.\n",
    "\n",
    "    Parameters:\n",
    "    file_safe (bool): If True, returns time formatted for file naming (replaces ':' with '_').\n",
    "\n",
    "    Returns:\n",
    "    str: Current time formatted as 'HH:MM:SS' or 'DD_HH_MM_SS' if file_safe is True.\n",
    "    \"\"\"\n",
    "    if file_safe:\n",
    "        return datetime.datetime.now().strftime('%d:%H:%M:%S').replace(\":\", \"_\")\n",
    "    else:\n",
    "        return datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "def count_files_in_directory(path: Union[str, 'LiteralString']) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of files in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "    path (Union[str, 'LiteralString']): The file path to the directory whose contents are to be counted.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If path is not a directory or does not exist.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of files in the specified directory.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"{path} is not a directory.\")\n",
    "    return len(os.listdir(path))\n",
    "\n",
    "\n",
    "def move_duplicate_files() -> list:\n",
    "    \"\"\"\n",
    "    Identify and remove duplicate images.\n",
    "    \n",
    "    Sometimes it's possible that a given image being added to the training set has already been added, either to the same folder or to the validation folder. This can happen through user error or random chance. This function hashes all image files in the training and holdout folders, looks for duplicates, and moves the files.\n",
    "    \n",
    "    Note: Do not look for duplicates in the Unlabeled folder. Unlabeled images with low confidence are manually classified and copied into the training folder to improve model accuracy in future runs.\n",
    "    \n",
    "    :return: A list of file paths that were moved. The function's primary activity occurs via side effects.\n",
    "    \"\"\"\n",
    "    file_path_list = []\n",
    "    file_hash_data = pl.DataFrame(schema={\"file_path\": str, \"file_hash\": str})\n",
    "\n",
    "    for outer_dir in [\"Labeled\", \"Holdout\"]:\n",
    "        for inner_dir in [\"Female\", \"Male\"]:\n",
    "            dir_path = os.path.join(data_dir, outer_dir, inner_dir)\n",
    "            if not os.path.exists(dir_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Directory does not exist: {dir_path}\"\n",
    "                )\n",
    "\n",
    "            with os.scandir(dir_path) as entries:\n",
    "                for entry in entries:\n",
    "                    file_path_list.append(entry.path)\n",
    "                    hasher = hashlib.sha512()\n",
    "                    with open(entry.path, 'rb') as file_entry:\n",
    "                        content = file_entry.read()\n",
    "                        hasher.update(content)\n",
    "                        this_entry_hash = hasher.hexdigest()\n",
    "                        file_hash_data = pl.concat(\n",
    "                            [file_hash_data, pl.DataFrame({\"file_path\": entry.path, \"file_hash\": this_entry_hash})])\n",
    "\n",
    "    dupe_file_paths = list(set(file_hash_data.to_series().to_list()) - set(\n",
    "        file_hash_data.sort(\"file_path\").group_by(\"file_hash\").last().select(\n",
    "            \"file_path\").to_series().to_list()))\n",
    "    for this_file_path in dupe_file_paths:\n",
    "        os.replace(this_file_path, os.path.join(data_dir, \"working_folder/temp\", os.path.basename(this_file_path)))\n",
    "    if len(dupe_file_paths) > 0:\n",
    "        print(f\"Moved {len(dupe_file_paths)} duplicate files\")\n",
    "    else:\n",
    "        print(\"No duplicate files found\")\n",
    "    return dupe_file_paths\n",
    "\n",
    "\n",
    "def get_dataset_structure_hash() -> str:\n",
    "    \"\"\"\n",
    "    Get list of all training and holdout files. Uses data_dir to define base directory.\n",
    "\n",
    "    Returns:\n",
    "    str: SHA512 hash of the file names of all training and holdout files.\n",
    "    \"\"\"\n",
    "    file_path_list = []\n",
    "\n",
    "    for outer_dir in [\"Labeled\", \"Holdout\"]:\n",
    "        for inner_dir in [\"Female\", \"Male\"]:\n",
    "            dir_path = os.path.join(data_dir, outer_dir, inner_dir)\n",
    "\n",
    "            if not os.path.exists(dir_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Directory does not exist: {dir_path}\"\n",
    "                )\n",
    "\n",
    "            with os.scandir(dir_path) as entries:\n",
    "                for entry in entries:\n",
    "                    file_path_list.append(entry.path)\n",
    "    file_path_list.sort()\n",
    "    file_path_list = \"\".join(file_path_list)\n",
    "    file_path_list_hash = hashlib.sha512(file_path_list.encode()).hexdigest()\n",
    "    return file_path_list_hash\n",
    "\n",
    "\n",
    "def plot_training_progress(train_acc: list,\n",
    "                           train_loss: list,\n",
    "                           val_acc: list,\n",
    "                           validation_loss: list,\n",
    "                           title=\"Model results\"):\n",
    "    \"\"\"\n",
    "    Plot training-vs-testing accuracy and loss for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        train_acc (list): List of training accuracy values from each epoch, must be same length as val_acc\n",
    "        train_loss (list): List of training loss values from each epoch, must be same length as val_loss\n",
    "        val_acc (list): List of validation accuracy values from each epoch, must be same length as train_acc\n",
    "        validation_loss (list): List of validation loss values from each epoch, must be same length as train_loss\n",
    "        title (str): Plot title\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the lengths of accuracy or loss lists don't match each other\n",
    "    \"\"\"\n",
    "    if not (len(train_acc) == len(val_acc) and len(train_loss) == len(validation_loss)) and len(train_acc) > 0 and len(\n",
    "            train_loss) > 0:\n",
    "        raise ValueError(\"Lengths of training and validation lists must match\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    axs[0].plot(train_acc,\n",
    "                label=\"Training accuracy\",\n",
    "                color=train_color,\n",
    "                linewidth=3)\n",
    "    axs[0].plot(val_acc,\n",
    "                label=\"Validation accuracy\",\n",
    "                color=val_color,\n",
    "                linewidth=3)\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(train_loss,\n",
    "                label=\"Training loss\",\n",
    "                color=train_color,\n",
    "                linewidth=3)\n",
    "    axs[1].plot(validation_loss,\n",
    "                label=\"Validation loss\",\n",
    "                color=val_color,\n",
    "                linewidth=3)\n",
    "    axs[1].set_title(\"Loss\")\n",
    "    axs[1].set(ylim=(0, None))\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_occupation(image_data_frame: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds an 'occupation' column to the DataFrame by first neutralizing gender-specific terms \n",
    "    in the 'file_path' column and then extracting the occupation.\n",
    "\n",
    "    The function first removes 'female_' or 'male_' from the 'file_path', then assumes that \n",
    "    the occupation is embedded in the modified file path string, following a specific pattern \n",
    "    (fourth underscore-separated value after removal of gender terms).\n",
    "\n",
    "    If an occupation is not found (null value), it prints a warning with the file path.\n",
    "\n",
    "    Parameters:\n",
    "    image_data_frame (pl.DataFrame): A DataFrame with a column named 'file_path'.\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: The original DataFrame with an added 'occupation' column.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the 'file_path' column does not exist in the DataFrame.\n",
    "    \"\"\"\n",
    "    if 'file_path' not in image_data_frame.columns:\n",
    "        raise ValueError(\"The DataFrame must contain a 'file_path' column.\")\n",
    "\n",
    "    image_data_frame = image_data_frame.with_columns([\n",
    "        pl.col(\"file_path\")\n",
    "        .str.replace(pattern=\"(female_|male_)\", value=\"\")\n",
    "        .map_elements(lambda x: x.split(\".jpeg\")[0] if \".jpeg\" in x else x)\n",
    "        .str.extract(\"(?:[^_]*_){4}([^_]*)_\")\n",
    "        .str.replace(\"dietitian\", \"nutritionist\")\n",
    "        .alias(\"occupation\")\n",
    "    ])\n",
    "\n",
    "    # Check for null occupations and print warning\n",
    "    for row in image_data_frame.filter(pl.col(\"occupation\").is_null()).to_dicts():\n",
    "        print(f\"Warning: Null occupation found for file path: {row['file_path']}\")\n",
    "\n",
    "    return image_data_frame\n",
    "\n",
    "\n",
    "def label_images(csv_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads images from a specified CSV file and allows the user to label them interactively.\n",
    "\n",
    "    This function opens each image specified in the CSV file and displays it to the user. \n",
    "    The user can then label the image as 'Female', 'Male', or 'Discard' by pressing the corresponding \n",
    "    keys ('f', 'm', 'd' or '0', '1', '2'). The function logs the user's decision along with the image \n",
    "    path and writes this data to an output file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing the paths of the images to be labeled.\n",
    "    output_file (str): Path to the output CSV file where the image paths and labels will be saved.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If a key pressed is not among the specified keys ('f', 'm', 'd', '0', '1', '2').\n",
    "\n",
    "    Notes:\n",
    "    The function adds a text overlay to each image before displaying it, which indicates the keys \n",
    "    for labeling. If an image cannot be read, it logs the path with the label \"ERROR\".\n",
    "    \"\"\"\n",
    "    image_labels = []\n",
    "    image_files = pl.read_csv(csv_file).to_series().to_list()\n",
    "    image_files_length = len(image_files)\n",
    "\n",
    "    for image_path in image_files:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            cv2.putText(image, \"'F'emale, 'M'ale, 'D'iscard\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Image\", image)\n",
    "\n",
    "            key = cv2.waitKey(0)  # Wait for a key press\n",
    "            if key == ord('0') or key == ord('f'):\n",
    "                image_labels.append((image_path, \"Female\"))\n",
    "            elif key == ord('1') or key == ord('m'):\n",
    "                image_labels.append((image_path, \"Male\"))\n",
    "            elif key == ord('2') or key == ord('d'):\n",
    "                image_labels.append((image_path, \"Discard\"))\n",
    "            else:\n",
    "                raise ValueError(\"Only allowed values are 0, 1, 2, f, m, and d\")\n",
    "            image_index = image_files.index(image_path) + 1\n",
    "            print(f\"Image {image_index}/{image_files_length} classified as {chr(key)}\")\n",
    "\n",
    "            cv2.destroyAllWindows()\n",
    "        else:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            image_labels.append((image_path, \"ERROR\"))\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in image_labels:\n",
    "            f.write(f\"{item[0]},{item[1]}\\n\")\n",
    "\n",
    "\n",
    "@cache\n",
    "def calculate_normalization(custom_normalization: bool, hash_value: str) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of the dataset for normalization.\n",
    "\n",
    "    Parameters:\n",
    "    custom_normalization (bool): Determines if custom normalization values are to be calculated.\n",
    "    hash_value (str): The hash value of the dataset structure. Used for cache invalidation.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, List[float]]: A dictionary containing 'mean' and 'std', each a list of three floats.\n",
    "\n",
    "    Notes:\n",
    "    Function uses global variables batch_size and data_dir.\n",
    "    \"\"\"\n",
    "    # No-op to satisfy IDE checks\n",
    "    _ = len(hash_value)\n",
    "\n",
    "    if custom_normalization:\n",
    "        this_con = sqlite3.connect(\"genderative_ai_database.db\")\n",
    "        this_cur = this_con.cursor()\n",
    "        this_cur.execute(\"SELECT mean, std FROM normalization_cache WHERE hash_value = ?\", (hash_value,))\n",
    "        cached_result = this_cur.fetchone()\n",
    "        if cached_result:\n",
    "            normalization_dict = {'mean': json.loads(cached_result[0]), 'std': json.loads(cached_result[1])}\n",
    "        else:\n",
    "            unnormalized_dataset = ImageFolder(os.path.join(data_dir, 'Labeled'), transform=tensor_transform)\n",
    "            unnormalized_loader = DataLoader(unnormalized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            def calculate_mean_std(loader: DataLoader) -> (torch.Tensor, torch.Tensor):\n",
    "                \"\"\"\n",
    "                Calculate the mean and standard deviation of images in a DataLoader.\n",
    "            \n",
    "                Parameters:\n",
    "                loader (DataLoader): The DataLoader containing the dataset.\n",
    "            \n",
    "                Returns:\n",
    "                Tuple[torch.Tensor, torch.Tensor]: Mean and standard deviation tensors.\n",
    "                \"\"\"\n",
    "                mean_accumulator = 0.0\n",
    "                variance_accumulator = 0.0\n",
    "                for images, _ in loader:\n",
    "                    batch_samples = images.size(0)\n",
    "                    images = images.view(batch_samples, images.size(1), -1)\n",
    "                    mean_accumulator += images.mean(2).sum(0)\n",
    "                    variance_accumulator += images.var(2).sum(0)\n",
    "\n",
    "                mean_accumulator /= len(loader.dataset)\n",
    "                std_deviation = torch.sqrt(variance_accumulator / len(loader.dataset))\n",
    "                return mean_accumulator, std_deviation\n",
    "\n",
    "            dataset_mean, dataset_std = calculate_mean_std(unnormalized_loader)\n",
    "            normalization_dict = {'mean': dataset_mean.tolist(), 'std': dataset_std.tolist()}\n",
    "            # Store the new result in the cache\n",
    "            this_cur.execute(\"INSERT INTO normalization_cache (hash_value, mean, std) VALUES (?, ?, ?)\",\n",
    "                             (\n",
    "                             hash_value, json.dumps(normalization_dict['mean']), json.dumps(normalization_dict['std'])))\n",
    "            this_con.commit()\n",
    "            del unnormalized_dataset, unnormalized_loader\n",
    "            print(f\"Custom normalization values: {normalization_dict}\")\n",
    "    else:\n",
    "        normalization_dict = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "        print(\"Standard normalization values used\")\n",
    "    return normalization_dict\n",
    "\n",
    "\n",
    "def display_images_from_dataloader(dataloader: DataLoader, num_images: int = 8):\n",
    "    \"\"\"\n",
    "    Fetches a batch of images from the given DataLoader and displays them to user.\n",
    "    \n",
    "    Parameters:\n",
    "    dataloader (DataLoader): A PyTorch DataLoader object from which to fetch the images.\n",
    "    num_images (int): The number of images to display from the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def imshow(img: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Display an image by denormalizing and clipping its values.\n",
    "\n",
    "        This function takes a PyTorch tensor representing a grid of images, which have been normalized \n",
    "        previously, and performs denormalization to convert them back to their original color \n",
    "        space. It then clips the image values to be within the range [0, 1] to ensure \n",
    "        proper display. The image is displayed using matplotlib.\n",
    "\n",
    "        Parameters:\n",
    "        img (torch.Tensor): A PyTorch tensor representing a grid of images.\n",
    "        \"\"\"\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = normalization_values['std'] * img + normalization_values['mean']  # Denormalize\n",
    "        img = np.clip(img, 0, 1)  # Clip values to be between 0 and 1\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "    num_images = min(batch_size, num_images)\n",
    "    dataset_images, dataset_labels = next(iter(dataloader))\n",
    "    images_subset = dataset_images[:num_images]\n",
    "    imshow(torchvision.utils.make_grid(images_subset))\n",
    "    print(' '.join(f'{classes[dataset_labels[j]]:5s}' for j in range(num_images)))\n",
    "\n",
    "\n",
    "def predict_model_resnet(my_dataloader):\n",
    "    \"\"\"\n",
    "    Performs predictions using a ResNet model on a given dataloader.\n",
    "    \n",
    "    This function runs the ResNet model in evaluation mode and generates predictions for the input data. \n",
    "    It requires the dataloader to have a batch size of exactly 1. The function outputs a dataframe \n",
    "    with file paths, actual labels, predicted labels, probabilities of being female, and a flag indicating \n",
    "    correct predictions.\n",
    "    \n",
    "    Note: The sorting logic in the function is based on the condition of correct predictions and their confidence scores.\n",
    "    \n",
    "    Parameters:\n",
    "    my_dataloader (Dataloader): A PyTorch Dataloader object containing the data to be predicted. \n",
    "                                 The dataloader must have a batch size of 1.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If the dataloader's batch size is not 1.\n",
    "    \n",
    "    Returns:\n",
    "    Polars.DataFrame: A dataframe containing columns for file paths, labels, predictions, \n",
    "                      probabilities, and correct predictions.\n",
    "    \"\"\"\n",
    "    if my_dataloader.batch_size != 1:\n",
    "        raise ValueError(\"Predictions dataloader requires batch size of exactly 1\")\n",
    "    model_resnet.eval()\n",
    "    prediction_results = pl.DataFrame(schema={\"file_path\": str, \"label\": int, \"prediction\": int, \"prob_female\": float})\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(my_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_resnet(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            prob_female = torch.nn.functional.softmax(outputs, dim=1)[:, 0].tolist()\n",
    "            predicted = predicted.tolist()\n",
    "            labels = labels.tolist()\n",
    "            file_paths = my_dataloader.dataset.samples[i][0]\n",
    "            batch_results = pl.DataFrame({\"file_path\": file_paths,\n",
    "                                          \"label\": labels,\n",
    "                                          \"prediction\": predicted,\n",
    "                                          \"prob_female\": prob_female})\n",
    "            prediction_results = pl.concat([prediction_results, batch_results], how=\"vertical_relaxed\")\n",
    "        prediction_results = prediction_results.with_columns(\n",
    "            (1 * (pl.col(\"label\") == pl.col(\"prediction\"))).alias(\"correct_prediction\")\n",
    "        )\n",
    "        # I couldn't figure out the sorting logic so I asked SO\n",
    "        # https://stackoverflow.com/questions/77700489/how-to-perform-a-conditional-sort-in-polars/77700711\n",
    "        prediction_results = prediction_results.with_columns(\n",
    "            abs(pl.col(\"prob_female\") - 0.5).alias(\"confidence\")\n",
    "        ).sort([\n",
    "            (good_prediction := pl.col('label').eq(pl.col('prediction'))),\n",
    "            (good_prediction - 1) * pl.col('confidence'),\n",
    "            pl.col('confidence')\n",
    "        ])\n",
    "        return prediction_results\n",
    "\n",
    "\n",
    "def move_image(file_path, new_folder) -> None:\n",
    "    \"\"\"\n",
    "    Move image located at file_path to new_folder.\n",
    "    \n",
    "    :param file_path: Location of file to be moved.\n",
    "    :param new_folder: Folder where you want the file to be moved.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    new_path = os.path.join(data_dir, new_folder, os.path.basename(file_path))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    try:\n",
    "        shutil.move(file_path, new_path)\n",
    "        print(f\"File {file_path} successfully moved to {new_folder}\")\n",
    "        with open(\"file_move_history.txt\", 'a') as f:\n",
    "            f.write(f\"{file_path}\\n\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {file_path}: {e}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error: {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def move_labeled_images(image_label_dataframe_path: str = \"labels.csv\"):\n",
    "    image_label_dataframe = pl.read_csv(image_label_dataframe_path,\n",
    "                                        has_header=False,\n",
    "                                        new_columns=[\"file_path\", \"gender\"],\n",
    "                                        schema={\"file_path\": pl.Utf8, \"gender\": pl.Utf8}).with_columns(\n",
    "        # Only allow the three valid image labels\n",
    "        pl.col(\"gender\").cast(pl.Enum(categories=[\"Female\", \"Male\", \"Discard\"]))\n",
    "    )\n",
    "\n",
    "    for row in image_label_dataframe.iter_rows():\n",
    "        file_path, label = row\n",
    "        sub_folder = file_path.replace(data_dir, \"\", 1).strip(os.sep).split(os.sep)[0].lstrip()\n",
    "        sub_folder = data_dir + sub_folder\n",
    "        destination_folder = os.path.join(sub_folder, label)\n",
    "        current_folder = os.path.dirname(file_path)\n",
    "        # Check if the current folder is the same as the destination folder\n",
    "        if current_folder != destination_folder:\n",
    "            if os.path.isfile(file_path):\n",
    "                move_image(file_path, destination_folder)\n",
    "            else:\n",
    "                print(f\"Image not found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"File is already in the correct folder: {file_path}\")\n",
    "            with open(\"file_move_history.txt\", 'a') as f:\n",
    "                f.write(f\"{file_path}\\n\")\n",
    "\n",
    "\n",
    "def get_occupation_counts() -> pl.DataFrame:\n",
    "    \"\"\"Gets occupation count breakdown by gender for labeled and holdout data.\n",
    "    \n",
    "    This function walks through the labeled and holdout image subfolders, extracts \n",
    "    the occupation from the file path, and aggregates a count by occupation and \n",
    "    gender. It then pivots the data to compare female and male counts by occupation.\n",
    "    \n",
    "    The output DataFrame contains:\n",
    "    - subfolder: Labeled or Holdout \n",
    "    - Occupation\n",
    "    - Count by gender\n",
    "    - Absolute difference in counts\n",
    "    - Percent difference \n",
    "    \n",
    "    Returns:\n",
    "    pl.DataFrame: Breakdown of occupation counts by gender for labeled and holdout data.\n",
    "    \"\"\"\n",
    "    subfolder_gender_occupation_count = pl.DataFrame(\n",
    "        schema={\"directory\": str, \"subfolder\": str, \"gender\": str, \"occupation\": str}\n",
    "    )\n",
    "\n",
    "    for this_subfolder in [\"labeled\", \"holdout\"]:\n",
    "        for this_gender in [\"female\", \"male\"]:\n",
    "            for root, dirs, files in os.walk(\n",
    "                    os.path.join(data_dir, this_subfolder, this_gender)\n",
    "            ):\n",
    "                if len(files) == 0:\n",
    "                    raise ValueError(f\"Empty directory: {root}\")\n",
    "                root = str(root).replace(data_dir, \"\", 1)\n",
    "                subfolder_gender_occupation_count.extend(\n",
    "                    add_occupation(\n",
    "                        pl.DataFrame(\n",
    "                            {\n",
    "                                \"directory\": [root] * len(files),\n",
    "                                \"subfolder\": [this_subfolder] * len(files),\n",
    "                                \"gender\": [this_gender] * len(files),\n",
    "                                \"file_path\": files,\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    .drop(\"file_path\")\n",
    "                    .sort([\"directory\", \"occupation\"])\n",
    "                )\n",
    "    subfolder_gender_occupation_summary = (\n",
    "        subfolder_gender_occupation_count.group_by(\n",
    "            [\"directory\", \"subfolder\", \"gender\", \"occupation\"]\n",
    "        )\n",
    "        .agg(pl.count())\n",
    "        .sort([\"directory\", \"occupation\"])\n",
    "    ).with_columns(\n",
    "        pl.col(\"count\").cast(pl.Int32)\n",
    "    )\n",
    "\n",
    "    subfolder_gender_occupation_summary = (subfolder_gender_occupation_summary.pivot(\n",
    "        values=\"count\",\n",
    "        index=[\"subfolder\", \"occupation\"],\n",
    "        columns=\"gender\",\n",
    "        aggregate_function=None\n",
    "    ))\n",
    "\n",
    "    return (subfolder_gender_occupation_summary.with_columns(\n",
    "        abs(pl.col(\"male\").sub(pl.col(\"female\"))).alias(\"gender_difference\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"gender_difference\").truediv((pl.col(\"female\").add(pl.col(\"male\")))).alias(\"gender_difference_pct\"))\n",
    "    ).sort([\"subfolder\", \"gender_difference_pct\", \"gender_difference\", \"female\"], descending=True)\n",
    "\n",
    "\n",
    "def calculate_metrics(subset_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Calculates model evaluation metrics on a subset of predictions.\n",
    "    \n",
    "    This function takes a DataFrame containing ground truth labels, predictions,\n",
    "    and predicted probabilities. It calculates a suite of common classification\n",
    "    evaluation metrics.\n",
    "    \n",
    "    The metrics calculated are:\n",
    "    - Accuracy\n",
    "    - ROC AUC\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "    - Matthews Correlation Coefficient\n",
    "    - Cohen's Kappa\n",
    "    - PR AUC\n",
    "    - Brier Score\n",
    "    - Hamming Loss\n",
    "    - Jaccard Loss\n",
    "    - Number of samples\n",
    "    \n",
    "    Parameters:\n",
    "    subset_df (pl.DataFrame): DataFrame with columns for labels, predictions, probabilities\n",
    "    \n",
    "    Returns:\n",
    "    pl.DataFrame: DataFrame containing metric values, along with the occupation\n",
    "    \"\"\"\n",
    "    y_true = subset_df['label']\n",
    "    y_pred = subset_df['prediction']\n",
    "    y_prob = subset_df['prob_male']\n",
    "\n",
    "    this_occupation = subset_df['occupation'].unique()\n",
    "    if len(this_occupation) > 1:\n",
    "        this_occupation = \"OVERALL\"\n",
    "\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(y_true, y_prob)\n",
    "    precision = sklearn.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sklearn.metrics.recall_score(y_true, y_pred)\n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    matthews_corrcoef = sklearn.metrics.matthews_corrcoef(y_true, y_pred)\n",
    "    cohen_kappa = sklearn.metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    precision_curve, recall_curve, _ = sklearn.metrics.precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = sklearn.metrics.auc(recall_curve, precision_curve)\n",
    "    brier_score = sklearn.metrics.brier_score_loss(y_true, y_prob)\n",
    "    hamming_loss = sklearn.metrics.hamming_loss(y_true, y_pred)\n",
    "    jaccard_loss = 1 - sklearn.metrics.jaccard_score(y_true, y_pred)\n",
    "    n_samples = subset_df.shape[0]\n",
    "\n",
    "    return pl.DataFrame({\n",
    "        \"occupation\": this_occupation,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"matthews_corrcoef\": matthews_corrcoef,\n",
    "        \"cohen_kappa\": cohen_kappa,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"brier_score\": brier_score,\n",
    "        \"hamming_loss\": hamming_loss,\n",
    "        \"jaccard_loss\": jaccard_loss,\n",
    "        \"n\": n_samples\n",
    "    })\n",
    "\n",
    "\n",
    "# Helper classes\n",
    "class ImbalancedClassesException(ValueError):\n",
    "    \"\"\"Exception raised when classes must be perfectly balanced and are not.\"\"\"\n",
    "\n",
    "\n",
    "class Colors(NamedTuple):\n",
    "    train: str\n",
    "    val: str\n",
    "    unlabeled: str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:25:25.949516600Z",
     "start_time": "2024-02-01T20:25:25.850477500Z"
    }
   },
   "id": "e06b185676f25570",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Config variables\n",
    "colors = Colors(train=\"#ff130f\", val=\"#7fff0f\", unlabeled=\"#0fefff\")\n",
    "train_color = colors.train\n",
    "val_color = colors.val\n",
    "unlabeled_color = colors.unlabeled\n",
    "\n",
    "data_dir: str = 'H:/Photos/AI/'\n",
    "batch_size: int = 30\n",
    "classes: tuple[str, str] = ('Female', 'Male')\n",
    "use_custom_normalization: bool = True\n",
    "num_epochs: int = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:25:26.872156200Z",
     "start_time": "2024-02-01T20:25:26.848152100Z"
    }
   },
   "id": "eea0dee7aad1787",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate files found\n"
     ]
    }
   ],
   "source": [
    "dataset_structure_hash = get_dataset_structure_hash()\n",
    "moved_file_path_list = move_duplicate_files()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:23:39.498436900Z",
     "start_time": "2024-02-01T20:22:20.630973500Z"
    }
   },
   "id": "560270e450cdff61",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc29b10a77bd6120",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:25:30.666269Z",
     "start_time": "2024-02-01T20:25:30.423952600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_counts = {category: {gender: count_files_in_directory(\n",
    "    os.path.join(data_dir, category, gender))\n",
    "    for gender in classes}\n",
    "    for category in ['Labeled', 'Holdout']}\n",
    "file_counts.update({'Unlabeled': count_files_in_directory(os.path.join(data_dir, \"Unlabeled/Unlabeled\"))})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Training\\nFemale', 'Training\\nMale',\n",
    "        'Holdout\\nFemale', 'Holdout\\nMale',\n",
    "        'Unlabeled'],\n",
    "       [file_counts['Labeled']['Female'],\n",
    "        file_counts['Labeled']['Male'],\n",
    "        file_counts['Holdout']['Female'],\n",
    "        file_counts['Holdout']['Male'],\n",
    "        file_counts['Unlabeled']],\n",
    "       color=[train_color, train_color, val_color, val_color, unlabeled_color])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Training and validation image counts\")\n",
    "plt.show()\n",
    "\n",
    "if file_counts['Labeled']['Female'] != file_counts['Labeled']['Male']:\n",
    "    female_count = file_counts['Labeled']['Female']\n",
    "    male_count = file_counts['Labeled']['Male']\n",
    "    raise ImbalancedClassesException(\n",
    "        f'Imbalance detected: {female_count} female images vs {male_count} male images')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "shape: (42, 6)\n┌───────────┬──────────────┬────────┬──────┬───────────────────┬───────────────────────┐\n│ subfolder ┆ occupation   ┆ female ┆ male ┆ gender_difference ┆ gender_difference_pct │\n│ ---       ┆ ---          ┆ ---    ┆ ---  ┆ ---               ┆ ---                   │\n│ str       ┆ str          ┆ i32    ┆ i32  ┆ i32               ┆ f64                   │\n╞═══════════╪══════════════╪════════╪══════╪═══════════════════╪═══════════════════════╡\n│ labeled   ┆ nurse        ┆ 2722   ┆ 2722 ┆ 0                 ┆ 0.0                   │\n│ labeled   ┆ pharmacist   ┆ 2557   ┆ 2557 ┆ 0                 ┆ 0.0                   │\n│ labeled   ┆ doctor       ┆ 2457   ┆ 2457 ┆ 0                 ┆ 0.0                   │\n│ labeled   ┆ nutritionist ┆ 1334   ┆ 1334 ┆ 0                 ┆ 0.0                   │\n│ …         ┆ …            ┆ …      ┆ …    ┆ …                 ┆ …                     │\n│ holdout   ┆ paramedic    ┆ 318    ┆ 318  ┆ 0                 ┆ 0.0                   │\n│ holdout   ┆ psychologist ┆ 305    ┆ 305  ┆ 0                 ┆ 0.0                   │\n│ holdout   ┆ dentist      ┆ 263    ┆ 263  ┆ 0                 ┆ 0.0                   │\n│ holdout   ┆ phlebotomist ┆ 214    ┆ 214  ┆ 0                 ┆ 0.0                   │\n└───────────┴──────────────┴────────┴──────┴───────────────────┴───────────────────────┘",
      "text/html": "<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (42, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subfolder</th><th>occupation</th><th>female</th><th>male</th><th>gender_difference</th><th>gender_difference_pct</th></tr><tr><td>str</td><td>str</td><td>i32</td><td>i32</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;labeled&quot;</td><td>&quot;nurse&quot;</td><td>2722</td><td>2722</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;pharmacist&quot;</td><td>2557</td><td>2557</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;doctor&quot;</td><td>2457</td><td>2457</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;nutritionist&quot;</td><td>1334</td><td>1334</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;paramedic&quot;</td><td>1007</td><td>1007</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;social&quot;</td><td>852</td><td>852</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;veterinarian&quot;</td><td>764</td><td>764</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;psychologist&quot;</td><td>574</td><td>574</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;dentist&quot;</td><td>564</td><td>564</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;chiropractor&quot;</td><td>559</td><td>559</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;black&quot;</td><td>542</td><td>542</td><td>0</td><td>0.0</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;phlebotomist&quot;</td><td>500</td><td>500</td><td>0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;labeled&quot;</td><td>&quot;plumber&quot;</td><td>50</td><td>50</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;doctor&quot;</td><td>852</td><td>852</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;pharmacist&quot;</td><td>780</td><td>780</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;nurse&quot;</td><td>718</td><td>718</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;nutritionist&quot;</td><td>667</td><td>667</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;veterinarian&quot;</td><td>481</td><td>481</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;chiropractor&quot;</td><td>450</td><td>450</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;social&quot;</td><td>400</td><td>400</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;paramedic&quot;</td><td>318</td><td>318</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;psychologist&quot;</td><td>305</td><td>305</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;dentist&quot;</td><td>263</td><td>263</td><td>0</td><td>0.0</td></tr><tr><td>&quot;holdout&quot;</td><td>&quot;phlebotomist&quot;</td><td>214</td><td>214</td><td>0</td><td>0.0</td></tr></tbody></table></div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occupation_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:25:31.956707100Z",
     "start_time": "2024-02-01T20:25:31.762823300Z"
    }
   },
   "id": "fba4db0211e44067",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "normalization_values = calculate_normalization(use_custom_normalization, dataset_structure_hash)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:25:36.640437Z"
    }
   },
   "id": "2774d1fcab1bba50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load train/test images in Pytorch format\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Images are already 512x512, but it's good to be sure\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalization_values['mean'], std=normalization_values['std'])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(data_dir, 'Labeled'), transform=transformations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = ImageFolder(os.path.join(data_dir, 'Holdout'), transform=transformations)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "display_images_from_dataloader(train_loader, num_images=16)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.538587700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5734761e01a6b298"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "# Available sizes are 18, 34, 50, 101, 152\n",
    "model_resnet = models.resnet152(weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2)\n",
    "model_resnet.name = \"ResNet152v2\"\n",
    "#model_resnet = models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "#model_resnet.name = \"ResNet18\"\n",
    "\n",
    "# Freeze the layers except the final layer\n",
    "for param in model_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully connected layer for binary classification\n",
    "num_features = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_resnet = model_resnet.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_resnet.fc.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.542602700Z"
    }
   },
   "id": "154b6f22a12e1b9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet_train_epoch_loss = []\n",
    "resnet_train_epoch_accuracy = []\n",
    "resnet_validation_epoch_loss = []\n",
    "resnet_validation_epoch_accuracy = []\n",
    "model_training_start_time = datetime.datetime.now().timestamp()\n",
    "\n",
    "# Fine-tune the ResNet model\n",
    "for epoch in range(num_epochs):\n",
    "    model_resnet.train()\n",
    "    print(f\"Epoch {epoch + 1} initiated\")\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        if i % 250 == 0:\n",
    "            print(f\"Training batch {i + 1}/{len(train_loader)} ({current_time_only()})\")\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    resnet_train_epoch_loss.append(running_loss / len(train_loader))\n",
    "    resnet_train_epoch_accuracy.append(correct_train / total_train)\n",
    "\n",
    "    # Validation\n",
    "    model_resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        print(f\"Validation started ({current_time_only()})\")\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_resnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate validation loss and accuracy for the epoch\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_accuracy = correct / total\n",
    "    resnet_validation_epoch_loss.append(epoch_val_loss)\n",
    "    resnet_validation_epoch_accuracy.append(epoch_val_accuracy)\n",
    "\n",
    "    # Extra fancy console output\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(f\"{'*' * 10} EPOCH {epoch + 1} RESULTS {'*' * 10}\".center(50))\n",
    "    print(\"*\" * 50)\n",
    "    print(f'\\033[94mTrain Loss: {running_loss / len(train_loader):.3f}\\033[0m, '\n",
    "          f'\\033[92mValidation Loss: {epoch_val_loss:.3f}\\033[0m, '\n",
    "          f'\\033[93mTraining Accuracy: {100 * correct_train / total_train:.2f}%\\033[0m, '\n",
    "          f'\\033[91mValidation Accuracy: {100 * epoch_val_accuracy:.2f}%\\033[0m')\n",
    "    print(\"*\" * 50 + \"\\n\")\n",
    "\n",
    "    # Write data to .csv as a backup to the sqlite database\n",
    "    with open(\"training_log.csv\", 'a') as f:\n",
    "        # Save epoch metrics\n",
    "        # Column names: model_training_start_time, model_name, batch_size, epoch, train_loss,\n",
    "        # val_loss, train_accuracy, val_accuracy, epoch_completion_timestamp, dataset_structure_hash\n",
    "        f.write(\n",
    "            f\"{model_training_start_time},{model_resnet.name},{batch_size},{epoch + 1},{running_loss / len(train_loader)},\"\n",
    "            f\"{epoch_val_loss},{100 * correct_train / total_train},{100 * epoch_val_accuracy},\"\n",
    "            f\"{datetime.datetime.now().timestamp()},{dataset_structure_hash}\\n\")\n",
    "\n",
    "    # Prepare data for insertion\n",
    "    data_to_insert = (model_training_start_time, model_resnet.name, batch_size, epoch + 1,\n",
    "                      running_loss / len(train_loader), epoch_val_loss,\n",
    "                      100 * correct_train / total_train, 100 * epoch_val_accuracy,\n",
    "                      datetime.datetime.now().timestamp(), dataset_structure_hash)\n",
    "\n",
    "    # Prepare INSERT INTO statement\n",
    "    insert_query = \"\"\"INSERT INTO training_log (model_training_start_time, model_name, batch_size, epoch, train_loss, \n",
    "                    val_loss, train_accuracy, val_accuracy, epoch_completion_timestamp, dataset_structure_hash) \n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\"\"\"\n",
    "\n",
    "    con = sqlite3.connect(\"genderative_ai_database.db\")\n",
    "    cur = con.cursor()\n",
    "    cur.execute(insert_query, data_to_insert)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "print(f'Finished Training ({current_time_only()})')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.546614300Z"
    }
   },
   "id": "a91ff7e6685697f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model_resnet, f\"{model_resnet.name}_{int(datetime.datetime.now().timestamp())}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.549615900Z"
    }
   },
   "id": "6128c612070ed76b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load most recent model\n",
    "#model_files = [f for f in os.listdir() if f.startswith(\"ResNet\")]\n",
    "#model_files.sort()\n",
    "#model_resnet = torch.load(model_files[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.554622800Z"
    }
   },
   "id": "c39019187e0861b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_training_progress(train_acc=resnet_train_epoch_accuracy,\n",
    "                       train_loss=resnet_train_epoch_loss,\n",
    "                       val_acc=resnet_validation_epoch_accuracy,\n",
    "                       validation_loss=resnet_validation_epoch_loss,\n",
    "                       title=\"Fine-tuned model results\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:23:40.587152300Z",
     "start_time": "2024-02-01T20:23:40.560140100Z"
    }
   },
   "id": "618d925dcbba56e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e0361d29da0e40f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_log_schema = {\"key\": pl.UInt32,\n",
    "                       \"model_training_start_time\": pl.Float64,\n",
    "                       \"model_name\": pl.Utf8,\n",
    "                       \"batch_size\": pl.UInt64,\n",
    "                       \"epoch\": pl.UInt64,\n",
    "                       \"train_loss\": pl.Float64,\n",
    "                       \"val_loss\": pl.Float64,\n",
    "                       \"train_accuracy\": pl.Float64,\n",
    "                       \"val_accuracy\": pl.Float64,\n",
    "                       \"epoch_completion_timestamp\": pl.Float64,\n",
    "                       \"dataset_structure_hash\": pl.Utf8}\n",
    "\n",
    "con = sqlite3.connect(\"genderative_ai_database.db\")\n",
    "cur = con.cursor()\n",
    "training_log = pl.DataFrame(cur.execute(\"SELECT * FROM training_log\").fetchall(),\n",
    "                            schema=training_log_schema).with_columns(\n",
    "    (pl.col(\"epoch_completion_timestamp\") - pl.col(\"model_training_start_time\")).alias(\"training_duration\")\n",
    ")\n",
    "\n",
    "training_log_latest = (pl.DataFrame(cur.execute(\"\"\"\n",
    "    WITH latest_dataset_hash AS (\n",
    "        SELECT dataset_structure_hash\n",
    "        FROM training_log\n",
    "        ORDER BY epoch_completion_timestamp DESC\n",
    "        LIMIT 1\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM training_log\n",
    "    WHERE dataset_structure_hash IN (SELECT dataset_structure_hash FROM latest_dataset_hash);\n",
    "\"\"\").fetchall(),\n",
    "                                    schema=training_log_schema)\n",
    ".drop(\"key\")\n",
    ".with_columns(\n",
    "    (pl.col(\"epoch_completion_timestamp\") - pl.col(\"model_training_start_time\")).alias(\"training_duration\")\n",
    "))\n",
    "con.close()\n",
    "\n",
    "with pl.Config(verbose=True):\n",
    "    training_log_by_model = (\n",
    "        training_log\n",
    "        .lazy()\n",
    "        .group_by([\"model_name\", \"batch_size\", \"epoch\"])\n",
    "        .agg(\n",
    "            pl.median(\"train_loss\").alias(\"train_loss\"),\n",
    "            pl.median(\"val_loss\").alias(\"val_loss\"),\n",
    "            pl.median(\"train_accuracy\").alias(\"train_accuracy\"),\n",
    "            pl.median(\"val_accuracy\").alias(\"val_accuracy\"),\n",
    "            pl.median(\"training_duration\").alias(\"training_duration\"),\n",
    "            pl.count().alias(\"n\")\n",
    "        )\n",
    "        .sort('val_accuracy', descending=True)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "training_log_latest_by_model = (\n",
    "    training_log_latest\n",
    "    .lazy()\n",
    "    .group_by([\"model_name\", \"batch_size\", \"epoch\"])\n",
    "    .agg(\n",
    "        pl.median(\"train_loss\").alias(\"train_loss\"),\n",
    "        pl.median(\"val_loss\").alias(\"val_loss\"),\n",
    "        pl.median(\"train_accuracy\").alias(\"train_accuracy\"),\n",
    "        pl.median(\"val_accuracy\").alias(\"val_accuracy\"),\n",
    "        pl.median(\"training_duration\").alias(\"training_duration\"),\n",
    "        pl.count().alias(\"n\")\n",
    "    )\n",
    "    .sort('val_accuracy', descending=True)\n",
    "    .collect()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.565146900Z"
    }
   },
   "id": "3cf8105a0b5ef196"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=training_log_by_model, x=\"epoch\", y=\"val_accuracy\", hue=\"model_name\", style=\"model_name\",\n",
    "             markers=True)\n",
    "plt.title('Validation Accuracy by model and epoch (all training runs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Model Name', loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.569144200Z"
    }
   },
   "id": "f298a2883730ab2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=training_log_by_model, x=\"training_duration\", y=\"val_accuracy\", hue=\"model_name\", style=\"model_name\",\n",
    "             markers=True)\n",
    "plt.title('Validation accuracy by model and duration (all training runs)')\n",
    "plt.xlabel('Training duration (seconds)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Model Name', loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.573141500Z"
    }
   },
   "id": "475d92a5b2098ada"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=training_log_latest_by_model,\n",
    "             x=\"epoch\",\n",
    "             y=\"val_accuracy\",\n",
    "             hue=\"model_name\",\n",
    "             style=\"model_name\",\n",
    "             markers=True)\n",
    "plt.title('Validation Accuracy by model and epoch (current dataset only)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Model Name', loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.577139600Z"
    }
   },
   "id": "5e81675c98048666",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=training_log_latest_by_model, x=\"training_duration\", y=\"val_accuracy\", hue=\"model_name\",\n",
    "             style=\"model_name\",\n",
    "             markers=True)\n",
    "plt.title('Validation accuracy by model and duration (current dataset only)')\n",
    "plt.xlabel('Training duration (seconds)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Model Name', loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.580141400Z"
    }
   },
   "id": "f1c02c9cefe1fa6d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual image classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "950e97fb12788163"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_exploration_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "train_exploration_results = predict_model_resnet(train_exploration_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.582144300Z"
    }
   },
   "id": "9a15a9128c94cf12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_bad_guesses = (train_exploration_results\n",
    ".filter((pl.col(\"confidence\") < 0.1) | (pl.col(\"correct_prediction\") == 0))\n",
    ".sort([\n",
    "    (good_prediction := pl.col('label').eq(pl.col('prediction'))),\n",
    "    (good_prediction - 1) * pl.col('confidence'),\n",
    "    pl.col('confidence')\n",
    "]))\n",
    "\n",
    "with open(\"file_move_history.txt\", \"r\") as file:\n",
    "    moved_files = file.read().splitlines()\n",
    "\n",
    "train_bad_guess_file_paths = train_bad_guesses.select(\"file_path\").to_series().to_list()\n",
    "\n",
    "train_bad_guess_file_paths = [fp for fp in train_bad_guess_file_paths if fp not in moved_files]\n",
    "\n",
    "pl.DataFrame(train_bad_guess_file_paths, schema=[\"file_path\"]).write_csv(\"train_holdout_bad_guesses.csv\")\n",
    "\n",
    "print(f\"{len(train_bad_guess_file_paths)} files to be reclassified\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.585139400Z"
    }
   },
   "id": "947d6aee5aa2140b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#label_images(csv_file=\"train_holdout_bad_guesses.csv\", output_file=\"train_holdout_manual_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T20:23:40.772409600Z",
     "start_time": "2024-02-01T20:23:40.590138500Z"
    }
   },
   "id": "699bbe42832608b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#move_labeled_images(\"train_holdout_manual_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.595141200Z"
    }
   },
   "id": "ec16ef073072c42c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exploration_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "exploration_results = predict_model_resnet(exploration_loader).with_columns(\n",
    "    (1 - pl.col(\"prob_female\")).alias(\"prob_male\")\n",
    ")\n",
    "exploration_results = add_occupation(exploration_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.598139400Z"
    }
   },
   "id": "1f22dbf89b4f4c49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_bad_guesses = (exploration_results\n",
    ".filter((pl.col(\"confidence\") < 0.1) | (pl.col(\"correct_prediction\") == 0))\n",
    ".sort([\n",
    "    (good_prediction := pl.col('label').eq(pl.col('prediction'))),\n",
    "    (good_prediction - 1) * pl.col('confidence'),\n",
    "    pl.col('confidence')\n",
    "]))\n",
    "\n",
    "with open(\"file_move_history.txt\", \"r\") as file:\n",
    "    moved_files = file.read().splitlines()\n",
    "\n",
    "bad_guess_file_paths = cv_bad_guesses.select(\"file_path\").to_series().to_list()\n",
    "\n",
    "holdout_filtered_files = [fp for fp in bad_guess_file_paths if fp not in moved_files]\n",
    "print(f\"{len(holdout_filtered_files)} files to be reclassified\")\n",
    "pl.DataFrame(holdout_filtered_files, schema=[\"file_path\"]).write_csv(\"holdout_bad_guesses.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.601141800Z"
    }
   },
   "id": "81b1d9d0130f235a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#label_images(csv_file=\"holdout_bad_guesses.csv\", output_file=\"holdout_manual_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.604143100Z"
    }
   },
   "id": "71602292e2e4ae00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#move_labeled_images(\"holdout_manual_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.607142600Z"
    }
   },
   "id": "a687344b49ed6cc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CV stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b15b84cac2f5c5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pie([exploration_results.select(pl.mean(\"correct_prediction\")).to_series().to_list()[0],\n",
    "        1 - exploration_results.select(pl.mean(\"correct_prediction\")).to_series().to_list()[0]],\n",
    "       labels=[\"Correct\", \"Incorrect\"],\n",
    "       autopct=\"%1.1f%%\",\n",
    "       colors=[\"#76ffbf\", \"#ff8876\"])\n",
    "plt.title(\"Cross-validated accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.610141500Z"
    }
   },
   "id": "cac48ac782a5ae24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx_to_class = {v: k for k, v in val_dataset.class_to_idx.items()}\n",
    "\n",
    "accuracy_by_gender = (exploration_results.select([\"label\", \"correct_prediction\"])\n",
    ".group_by(\"label\")\n",
    ".mean()\n",
    ".with_columns(\n",
    "    pl.col(\"label\").replace(idx_to_class)\n",
    "))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(list(zip(*accuracy_by_gender.rows()))[0], list(zip(*accuracy_by_gender.rows()))[1], color=\"#76ffbf\")\n",
    "plt.title(\"Accuracy by gender\")\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.613142Z"
    }
   },
   "id": "ed7fa36ce328c292"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(exploration_results.group_by('occupation')\n",
    " .map_groups(calculate_metrics)\n",
    " .vstack(\n",
    "    calculate_metrics(exploration_results)).sort(\"f1_score\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.616142Z"
    }
   },
   "id": "ab56f74ff4c9d63f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exploration_results_confidence = (exploration_results.select(\n",
    "    pl.col(\"prob_female\").round(1),\n",
    "    pl.col(\"label\")\n",
    ")\n",
    ".group_by(\"prob_female\")\n",
    ".agg(\n",
    "    abs(pl.mean(\"label\") - 1)\n",
    ")\n",
    ").sort(\"prob_female\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(exploration_results_confidence[\"prob_female\"], exploration_results_confidence[\"label\"])\n",
    "ax.set(xlabel=\"Predicted probability of female\", ylabel=\"Actual frequency of female\", title=\"Prediction vs labels\")\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "ax.annotate(\"$R^2$ = {:.2f}\".format(\n",
    "    sklearn.metrics.r2_score(exploration_results_confidence[\"prob_female\"], exploration_results_confidence[\"label\"])),\n",
    "    (0, 1))\n",
    "ax.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.620140400Z"
    }
   },
   "id": "c225abb2e0abd89c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unlabeled images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6376f5ca025622f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_dataset = ImageFolder(os.path.join(data_dir, 'Unlabeled'), transform=tensor_transform)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "unlabeled_predictions = predict_model_resnet(unlabeled_loader)\n",
    "unlabeled_predictions.drop([\"label\", \"correct_prediction\"])\n",
    "unlabeled_predictions = add_occupation(unlabeled_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.624175700Z"
    }
   },
   "id": "1d4025090c232e05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Lowest-confidence images\")\n",
    "for img_path in unlabeled_predictions.sort(\"confidence\", descending=False).head(6).to_series(0):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    display(PIL.Image.open(img_path).resize((128, 128)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.628143200Z"
    }
   },
   "id": "605989ac63bb83a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bls_gender = (\n",
    "    pl.DataFrame({\"occupation\": [\"nutritionist\", \"doctor\", \"chiropractor\", \"pharmacist\", \"dentist\", \"paramedic\",\n",
    "                                 \"psychologist\", \"veterinarian\", \"social\", \"phlebotomist\", \"nurse\"],\n",
    "                  \"bls_percent_female\": [.880, .438, .255, .596, .366, .236, .750, .698, .815, .879, .879]})\n",
    "    .sort(\"bls_percent_female\")\n",
    "    .with_columns(\n",
    "        (1 - pl.col(\"bls_percent_female\")).alias(\"bls_percent_male\")\n",
    "    ))\n",
    "\n",
    "unlabeled_prediction_summary = (unlabeled_predictions\n",
    "                                .select([\"occupation\", \"prediction\"])\n",
    "                                .group_by(\"occupation\")\n",
    "                                .agg(\n",
    "    pl.mean(\"prediction\"),\n",
    "    pl.count().alias(\"n\")\n",
    ")\n",
    "                                .sort(\"prediction\", descending=True)\n",
    "                                .filter(pl.col(\"n\") > 99)\n",
    "                                .join(bls_gender, on=\"occupation\", how=\"left\"))\n",
    "\n",
    "(unlabeled_prediction_summary.select([\"occupation\", \"prediction\", \"bls_percent_male\"])\n",
    " .sort(\"prediction\",\n",
    "       descending=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.630142300Z"
    }
   },
   "id": "85dcff2aa006686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(unlabeled_prediction_summary[\"occupation\"].to_list(),\n",
    "       unlabeled_prediction_summary[\"prediction\"].to_list())\n",
    "\n",
    "ax.set_title(\"Male representation by occupation\")\n",
    "ax.set_ylabel(\"Percent male\")\n",
    "ax.set_xlabel(\"Occupation\")\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "plt.xticks(unlabeled_prediction_summary[\"occupation\"].to_list(), rotation='vertical')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.632144700Z"
    }
   },
   "id": "29800d0604c1de41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(unlabeled_prediction_summary[\"bls_percent_male\"], unlabeled_prediction_summary[\"prediction\"])\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
    "ax.annotate(\"$R^2$ = {:.2f}\".format(\n",
    "    sklearn.metrics.r2_score(unlabeled_prediction_summary[\"bls_percent_male\"],\n",
    "                             unlabeled_prediction_summary[\"prediction\"])),\n",
    "    (0.04, .9))\n",
    "ax.set_xlabel(\"Percent male according to BLS employment data\")\n",
    "ax.set_ylabel(\"Percent male according to generative AI\")\n",
    "ax.set_title(\"Gender representation, BLS vs. generative AI\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.635140700Z"
    }
   },
   "id": "2fd053be076e9f68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unlabeled_predictions.filter(pl.col(\"confidence\") < 0.1).sort(\"confidence\").select(\"file_path\").write_csv(\n",
    "    \"unlabeled_path_test.csv\")\n",
    "\n",
    "label_images(\"unlabeled_path_test.csv\", \"unlabeled_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-01T20:23:40.637141Z"
    }
   },
   "id": "36d6ec9ceb76741b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#move_labeled_images(\"unlabeled_labels.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7719fcaa3c19007"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
